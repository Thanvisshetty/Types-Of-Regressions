# Types Of Regressions
## âœ… 1. Linear Regression
ğŸ” What It Is:
Linear Regression is the most basic type of regression. It models a linear relationship between an independent variable (X) and a dependent variable (y).

ğŸ§  How It Works:
It finds the best-fitting straight line through the data points by minimizing the sum of squared differences (errors) between actual and predicted values (Least Squares Method).

ğŸ“Š Use Case Example:
Predicting salary based on years of experience.

âœ… Pros:
Easy to interpret

Fast to train

Works well with linearly related data

âŒ Cons:
Not suitable for nonlinear relationships

Sensitive to outliers

## âœ… 2. Multiple Linear Regression
ğŸ” What It Is:
Itâ€™s an extension of Linear Regression where more than one feature (Xâ‚, Xâ‚‚, ..., Xâ‚™) is used to predict the target variable.
 
ğŸ§  How It Works:
It fits a hyperplane in n-dimensional space using multiple predictors.

ğŸ“Š Use Case Example:
Predicting house prices based on area, number of bedrooms, and age.

âœ… Pros:
Can handle more complex problems

Better predictive power with multiple relevant inputs

âŒ Cons:
Needs more data

Multicollinearity can reduce performance

## âœ… 3. Polynomial Regression
ğŸ” What It Is:
Polynomial Regression models nonlinear relationships between input and output by adding polynomial terms.

ğŸ§  How It Works:
Transforms input features into polynomial features and fits a linear model to them.

ğŸ“Š Use Case Example:
Predicting the square of a number (nonlinear growth like y = xÂ²).

âœ… Pros:
Captures curved trends

Simple to implement

âŒ Cons:
Can overfit if degree is too high

Less interpretable

## âœ… 4. Ridge Regression (L2 Regularization)
ğŸ” What It Is:
Itâ€™s a regularized version of Linear Regression that adds a penalty to the loss function to reduce overfitting.
â€‹
 
ğŸ§  How It Works:
It penalizes large coefficients and reduces their magnitude while keeping all features.

ğŸ“Š Use Case Example:
Predicting target where features are highly correlated (multicollinearity problem).

âœ… Pros:
Handles multicollinearity well

Reduces overfitting

âŒ Cons:
Doesnâ€™t eliminate features (all are kept)

## âœ… 5. Lasso Regression (L1 Regularization)
ğŸ” What It Is:
Similar to Ridge, but Lasso can shrink some coefficients to zero, effectively selecting important features.

 âˆ£
ğŸ§  How It Works:
Adds a penalty equal to the absolute value of the magnitude of coefficients.

ğŸ“Š Use Case Example:
Predicting with many features, some of which are irrelevant.

âœ… Pros:
Performs feature selection

Prevents overfitting

âŒ Cons:
Can discard useful features

## âœ… 6. Logistic Regression
ğŸ” What It Is:
Used for classification, not regression. Despite the name, it predicts probabilities for binary classes (0 or 1).
 
ğŸ§  How It Works:
Applies a logistic (sigmoid) function to model the probability that y belongs to class 1.

ğŸ“Š Use Case Example:
Predicting pass/fail based on marks.

âœ… Pros:
Works well for binary classification

Outputs probabilities

âŒ Cons:
Doesnâ€™t handle non-linearity unless transformed

## âœ… 7. ElasticNet Regression
ğŸ” What It Is:
A hybrid of Ridge and Lasso that combines both L1 and L2 penalties.

ğŸ§  How It Works:
Balances the strengths of Ridge and Lasso, controlling shrinkage and feature selection.

ğŸ“Š Use Case Example:
When some features are highly correlated, and you also want feature selection.

âœ… Pros:
Balanced regularization

Feature selection + shrinkage

âŒ Cons:
Slightly more complex to tune

